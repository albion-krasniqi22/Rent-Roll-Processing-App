{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45410569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import io\n",
    "import pandas as pd\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "client = OpenAI(api_key=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43747bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_headers_info(df, message=\"\"):\n",
    "    \"\"\"\n",
    "    Print a message and then display the first few rows to see the headers visually.\n",
    "    \"\"\"\n",
    "    if message:\n",
    "        print(message)\n",
    "    display(HTML(df.head().to_html()))\n",
    "    \n",
    "    print(\"Columns:\", list(df.columns))\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"-\"*100)\n",
    "    print('\\n\\n')\n",
    "\n",
    "\n",
    "def load_excel_file(filepath):\n",
    "    \"\"\"\n",
    "    Load the Excel file into a pandas DataFrame with no header (header=None).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(filepath, sheet_name=0, header=None)\n",
    "        #print(f\"File '{filepath}' loaded successfully. Shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def read_top_rows(filepath, max_rows=10):\n",
    "    \"\"\"\n",
    "    Reads the top 'max_rows' from the Excel file (no header)\n",
    "    and converts them into a single text block.\n",
    "    \"\"\"\n",
    "    df_top = pd.read_excel(filepath, nrows=max_rows, header=None)\n",
    "    # Convert each row to a string\n",
    "    lines = []\n",
    "    for _, row in df_top.iterrows():\n",
    "        # For each row, convert columns to strings, then join\n",
    "        row_str = \" | \".join(str(x) for x in row if pd.notnull(x))\n",
    "        # e.g. \"Rent Roll with Lease Charges\"\n",
    "        lines.append(row_str.strip())\n",
    "    \n",
    "    # Combine lines into one block of text\n",
    "    top_text = \"\\n\".join(lines)\n",
    "    return top_text\n",
    "\n",
    "\n",
    "def extract_property_info_via_gpt(text_block, model=\"gpt-3.5-turbo\"):    \n",
    "    # Example system or user instructions\n",
    "    system_instructions = \"\"\"\n",
    "    You are an assistant specialized in reading the top text of a rent roll file.\n",
    "    Your task is to find the 'property name' and the 'as_of_date' from the text provided.\n",
    "    If the date is missing or invalid, set it to null. Return the result as JSON.\n",
    "    Schema:\n",
    "    {\n",
    "      \"property_name\": string or null,\n",
    "      \"as_of_date\": string in \"YYYY-MM-DD\" format or null\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    Text from the top of an Excel file:\n",
    "    --------------\n",
    "    {text_block}\n",
    "    --------------\n",
    "    Please parse the property name and the as-of date.\n",
    "    Return as JSON: {{ \"property_name\": \"...\", \"as_of_date\": \"YYYY-MM-DD\" }}\n",
    "    If you can't find them, set them to null.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_instructions},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract the content\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Attempt to parse JSON\n",
    "    try:\n",
    "        data = json.loads(content)\n",
    "        prop_name = data.get(\"property_name\")\n",
    "        as_of_date = data.get(\"as_of_date\")\n",
    "        return prop_name, as_of_date\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing JSON from GPT:\", e)\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def get_property_info(filepath):\n",
    "    \"\"\"\n",
    "    1. Read the top text lines from the Excel file.\n",
    "    2. Ask GPT for property name & as_of_date.\n",
    "    3. Return them (or None if not found).\n",
    "    \"\"\"\n",
    "    top_text = read_top_rows(filepath, max_rows=10)\n",
    "    # Now call GPT to parse\n",
    "    property_name, as_of_date = extract_property_info_via_gpt(top_text)\n",
    "\n",
    "    print(\"Property Name:\", property_name)\n",
    "    print(\"As of date:\", as_of_date)\n",
    "    \n",
    "    return property_name, as_of_date\n",
    "\n",
    "\n",
    "def identify_header_candidates(sheet_data, keywords):\n",
    "    \"\"\"\n",
    "    Find rows that contain multiple keyword matches as candidate header rows.\n",
    "    Returns a DataFrame of candidate header rows (if any).\n",
    "    \"\"\"\n",
    "    # Convert all cells to lowercase strings (handle NaNs)\n",
    "    normalized_data = sheet_data.applymap(lambda x: str(x).lower() if pd.notnull(x) else '')\n",
    "    \n",
    "    # Count how many keywords appear in each row\n",
    "    normalized_data['keyword_count'] = normalized_data.apply(\n",
    "        lambda row: sum(row.str.contains('|'.join(keywords), regex=True)), axis=1\n",
    "    )\n",
    "    \n",
    "    # Candidate rows: containing >=3 hits from the keyword list\n",
    "    header_candidates = normalized_data[normalized_data['keyword_count'] >= 3]\n",
    "    \n",
    "    return header_candidates\n",
    "\n",
    "\n",
    "def merge_and_select_first_header_to_bottom(df, keyword_column, keywords):\n",
    "    \"\"\"\n",
    "    If consecutive rows might each contain partial headers, \n",
    "    this attempts to merge them to create a single best header row.\n",
    "    Returns a single-row DataFrame representing the merged header.\n",
    "    \"\"\"\n",
    "    df = df.sort_index()\n",
    "    merged_header = None\n",
    "    final_header = None\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if merged_header is None:\n",
    "            merged_header = row\n",
    "            final_header = row\n",
    "            continue\n",
    "\n",
    "        # If the row is right after the merged_header, try merging\n",
    "        if idx - merged_header.name == 1:\n",
    "            combined_row = merged_header[:-1] + \" \" + row[:-1]\n",
    "            combined_keyword_count = sum(combined_row.str.contains('|'.join(keywords), regex=True))\n",
    "            \n",
    "            # If the newly combined row has more keyword hits than the old one, \n",
    "            # update the final_header\n",
    "            if combined_keyword_count > merged_header[keyword_column]:\n",
    "                row[:-1] = combined_row\n",
    "                row[keyword_column] = combined_keyword_count\n",
    "                final_header = row\n",
    "            continue\n",
    "\n",
    "        # If not consecutive, break out\n",
    "        break\n",
    "\n",
    "    if final_header is not None:\n",
    "        return pd.DataFrame([final_header])\n",
    "    else:\n",
    "        return pd.DataFrame([])\n",
    "\n",
    "\n",
    "def standardization_instructions():\n",
    "    \"\"\"\n",
    "    Returns the standardization prompt to be sent to GPT.\n",
    "    \"\"\"\n",
    "    instructions_prompt = \"\"\"\n",
    "    We aim to standardize headers across multiple documents to ensure consistency and ease of processing. Below are examples of how various column names might appear in different documents and the standardized format we want to achieve:\n",
    "\n",
    "    Standardized Column Headers:\n",
    "    - Unit No.: Includes variations such as:\n",
    "        - \"Unit\", \"Unit Id\", \"Unit Number\", \"bldg-unit\", \"apt #\", \"apt number\"\n",
    "        - Columns containing the substring \"Id\" can be mapped to \"Unit\" only if no other \"Unit\"-related columns (e.g., \"Unit\", \"Unit Number\", etc.) are available.\n",
    "        - Avoid \"Unit No.\": Clearly specifies that this rule applies only to the \"Unit\" column and not to \"Unit No.\".\n",
    "    - Floor Plan Code: Includes variations like \"Floor Plan\", \"Plan Code\", \"Floorplan\", \"Unit Type\", Bd/Ba, \"Type\"\n",
    "    - Net sf: Includes variations like \"Sqft\", \"Unit Sqft\", \"Square Feet\", \"Sq. Ft.\"\n",
    "    -  Occupancy Status / Code: Includes variations like \"Unit Status\", \"Lease Status\", \"Occupancy\", \"Unit/Lease Status\"\n",
    "    - Market Rent: Includes variations like \"Market Rent\", \"Market + Addl.\", 'Gross Market Rent'\n",
    "    - Lease Start Date: Includes variations like \"Lease Start\", \"Lease Start Date\", \"Start of Lease\" (not lease name)\n",
    "    - Lease Expiration: Includes variations like \"Lease End\", \"Lease End Date\", \"Lease Expiration Date\"\n",
    "    - Move In Date: Includes variations like \"Move-In\", \"Move In Date\", \"Move In\"\n",
    "    - Move Out Date: Includes variations like \"Move-Out\", \"Move Out Date\", \"Move Out\"\n",
    "    - Charge Codes: Includes variations like \"Trans Code\", \"Charge Codes\", \"Description\"\n",
    "    - Amount: these are charges in dollar amount (which is different from charge code), Charges or credits\n",
    "\n",
    "    Examples of Standardized Headers:\n",
    "    Unit No., Floor Plan Code, Sqft, Occupancy Status, Market Rent, Lease Start Date, Lease Expiration, Move In Date, Move-Out Date, Charge Codes\n",
    "\n",
    "    Task:\n",
    "    Your task is to analyze the headers provided in a list and map each header to its corresponding standardized column name. If a header does not match any standardized category, retain it as-is.\n",
    "\n",
    "    Key Details:\n",
    "    1. The input is a list of column names.\n",
    "    2. The output must be a list of the same size, with each header mapped to its standardized name or retained as-is if no match is found.\n",
    "    3. Be mindful of slight differences in naming, abbreviations, or spacing in headers. Use the examples above as a reference for mapping.\n",
    "    4. If a header is unclear or does not match a category, make an educated guess or retain the original formatting with corrections for consistency.\n",
    "    5. If a specific rule or example is not provided, update the header format to follow Pascal Case and ensure clarity. Apply your best judgment to map headers to the standardized list or format them consistently while preserving their original intent.\n",
    "\n",
    "    Task:\n",
    "    1. Standardize the provided headers according to the categories above.\n",
    "    2. Return the result as a JSON object with a key 'standardized_headers' containing the list of standardized headers.\n",
    "    3. Preserve empty strings as they are.\n",
    "    4. Apply consistent formatting (Pascal Case, clarity, etc.)\n",
    "    5. If no clear standardization exists, keep the original header.\n",
    "\n",
    "    Example Input:\n",
    "    ['unit', 'floorplan', 'sqft', 'unit/lease status']\n",
    "\n",
    "    Example Output:\n",
    "    {\"standardized_headers\": [\"Unit No.\", \"Floor Plan Code\", \"Net sf\", \"Occupancy Status / Code\"]}\n",
    "    \"\"\"\n",
    "    return instructions_prompt\n",
    "\n",
    "\n",
    "def gpt_model(instructions_prompt, header, client):\n",
    "    headers_str = \", \".join(repr(h) for h in header)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Standardize these headers: {headers_str}\"}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "\n",
    "    response_content = response.choices[0].message.content\n",
    "    try:\n",
    "        standardized_headers = json.loads(response_content)['standardized_headers']\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        try:\n",
    "            standardized_headers = eval(response_content)\n",
    "        except:\n",
    "            standardized_headers = None\n",
    "\n",
    "    return standardized_headers\n",
    "\n",
    "\n",
    "def standardize_headers_with_retries(headers_to_standardize, instructions_prompt, client, max_retries=5):\n",
    "    attempt = 0\n",
    "    standardized_headers = None\n",
    "\n",
    "    while attempt < max_retries and standardized_headers is None:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            standardized_headers = gpt_model(instructions_prompt, headers_to_standardize, client)\n",
    "        except Exception as e:\n",
    "            standardized_headers = None\n",
    "\n",
    "        if standardized_headers is not None and len(standardized_headers) != len(headers_to_standardize):\n",
    "            standardized_headers = None\n",
    "\n",
    "    return standardized_headers\n",
    "\n",
    "\n",
    "\n",
    "def make_column_names_unique(column_names):\n",
    "    \"\"\"\n",
    "    If multiple columns end up with the same standardized name, \n",
    "    rename them uniquely (e.g., \"Unit No.\", \"Unit No._1\", \"Unit No._2\", etc.).\n",
    "    \"\"\"\n",
    "    cols = pd.Series(column_names).fillna('Unnamed').replace('', 'Unnamed')\n",
    "    duplicates = cols.duplicated(keep=False)\n",
    "    counts = {}\n",
    "\n",
    "    for idx, col in enumerate(cols):\n",
    "        if col in counts:\n",
    "            counts[col] += 1\n",
    "            cols[idx] = f\"{col}_{counts[col]}\"\n",
    "        else:\n",
    "            counts[col] = 0\n",
    "            if duplicates[idx]:\n",
    "                # If the very first time we see a duplicate, rename it\n",
    "                cols[idx] = f\"{col}_{counts[col]}\"\n",
    "    return cols.tolist()\n",
    "\n",
    "\n",
    "def drop_unnecessary_rows(df):\n",
    "    \"\"\"\n",
    "    Drop rows that contain no numeric values at all.\n",
    "    \"\"\"\n",
    "    before_count = len(df)\n",
    "    df = df.dropna(how='all')  # Drop any completely empty rows\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Filter rows that have at least one numeric value\n",
    "    numeric_filter = df.apply(lambda row: any(pd.to_numeric(row, errors='coerce').notnull()), axis=1)\n",
    "    df = df[numeric_filter].reset_index(drop=True)\n",
    "\n",
    "    after_count = len(df)\n",
    "    print(f\"Dropped {before_count - after_count} rows that contained no numeric values.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_breaking_point(data):\n",
    "    for index, row in data.iterrows():\n",
    "        if pd.notnull(row.get('Unit No.')):\n",
    "            lease_start_exists = 'Lease Start Date' in data.columns\n",
    "            rent_columns = [col for col in data.columns if 'rent' in col.lower()]\n",
    "            \n",
    "            net_sf = row.get('Net sf')\n",
    "            if pd.notnull(net_sf):\n",
    "                try:\n",
    "                    net_sf = float(str(net_sf).replace(',', ''))\n",
    "                except ValueError:\n",
    "                    net_sf = 0  # Default to 0 if conversion fails\n",
    "\n",
    "            if not (\n",
    "                ('Net sf' not in row or (pd.notnull(net_sf) and net_sf < 10000)) and\n",
    "                (any(\n",
    "                    pd.notnull(row[col]) and float(str(row[col]).replace(',', '')) < 10000\n",
    "                    for col in rent_columns\n",
    "                ) or (lease_start_exists and pd.notnull(row.get('Lease Start Date'))))\n",
    "            ):\n",
    "                return index\n",
    "\n",
    "\n",
    "            if 'Occupancy Status' in data.columns:\n",
    "                if pd.notnull(row.get('Occupancy Status / Code')) and not isinstance(row.get('Occupancy Status / Code'), str):\n",
    "                    return index\n",
    "\n",
    "            if 'Charge Codes' in data.columns:\n",
    "                if pd.notnull(row.get('Charge Codes')) and not isinstance(row.get('Charge Codes'), str):\n",
    "                    return index\n",
    "        else:\n",
    "            if pd.notnull(row.get('Net sf')) or pd.notnull(row.get('Market Rent')):\n",
    "                return index\n",
    "            if 'Charge Codes' in data.columns:\n",
    "                if pd.notnull(row.get('Charge Codes')) and row.isnull().all():\n",
    "                    return index\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def finalize_columns(df):\n",
    "    # Columns we expect/want to ensure exist in the final DataFrame\n",
    "    desired_columns = [\n",
    "        \"Unit No.\",\n",
    "        \"Floor Plan Code\",\n",
    "        \"Net sf\",\n",
    "        \"Occupancy Status / Code\",\n",
    "        'Enter \"F\" for Future Lease',\n",
    "        \"Market Rent\",\n",
    "        \"Lease Start Date\",\n",
    "        \"Lease Expiration\",\n",
    "        \"Lease Term (months)\",\n",
    "        \"Move In Date\",\n",
    "        \"Move Out Date\",\n",
    "    ]\n",
    "\n",
    "    # 0) Filter out rows that lack a valid \"Unit No.\"\n",
    "    #    i.e., exclude NaN or empty string\n",
    "    if \"Unit No.\" not in df.columns:\n",
    "        print(\"No 'Unit No.' column found. Cannot proceed with grouping or pivoting.\")\n",
    "        return df\n",
    "\n",
    "    # 1) Detect if we have the columns needed to pivot charges\n",
    "    has_charge_codes = (\"Charge Codes\" in df.columns and \"Amount\" in df.columns)\n",
    "    if has_charge_codes:\n",
    "        print(\"Detected scenario with 'Charge Codes' & 'Amount'. We will pivot them.\")\n",
    "    else:\n",
    "        print(\"No 'Charge Codes'/'Amount' in DataFrame => single-line scenario. No pivoting will occur.\")\n",
    "\n",
    "    # 2) Define columns to combine into the unique key\n",
    "    group_cols = [c for c in df.columns if c not in (\"Charge Codes\", \"Amount\")]\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Instead of unconditional forward fill, do a \"block-based\" approach:\n",
    "    # Mark each row that starts a new block whenever *any* group_col is non-null\n",
    "    df[\"_new_block\"] = df[group_cols].notna().any(axis=1).cumsum()\n",
    "\n",
    "    # Forward-fill within each block, so we don't just fill forever\n",
    "    df[group_cols] = df.groupby(\"_new_block\")[group_cols].ffill()\n",
    "\n",
    "    # Clean up helper columns\n",
    "    df.drop(columns=[\"_new_block\"], inplace=True, errors=\"ignore\")\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    # 3) Create a 'unique_key' for grouping\n",
    "    df[\"unique_key\"] = (\n",
    "        df[group_cols]\n",
    "        .astype(str)\n",
    "        .agg('|'.join, axis=1)\n",
    "        .fillna(\"EMPTY\")\n",
    "    )    \n",
    "\n",
    "    # 4) Group by 'unique_key' using an aggregator of 'first' for each non-pivot column\n",
    "    all_cols = df.columns.tolist()\n",
    "    aggregations = {col: \"first\" for col in all_cols if col not in [\"Charge Codes\", \"Amount\"]}\n",
    "\n",
    "    grouped = df.groupby(\"unique_key\", as_index=False).agg(aggregations)\n",
    "\n",
    "    # 6) Pivot 'Charge Codes' & 'Amount' if needed\n",
    "    if has_charge_codes:\n",
    "        pivoted_charges = (\n",
    "            df.pivot_table(\n",
    "                index=\"unique_key\",\n",
    "                columns=\"Charge Codes\",\n",
    "                values=\"Amount\",\n",
    "                aggfunc=\"sum\"\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        # Merge pivoted table with the grouped DataFrame\n",
    "        grouped = pd.merge(grouped, pivoted_charges, on=\"unique_key\", how=\"left\")\n",
    "\n",
    "        # Now drop 'unique_key' again if it re-appeared\n",
    "        if \"unique_key\" in grouped.columns:\n",
    "            grouped.drop(columns=[\"unique_key\"], inplace=True)\n",
    "\n",
    "    # 7) Ensure 'desired_columns' exist in the result\n",
    "    for col in desired_columns:\n",
    "        if col not in grouped.columns:\n",
    "            grouped[col] = None\n",
    "\n",
    "    # 8) Reorder columns:\n",
    "    #    \"Unit No.\" first, then the rest of desired_columns, then leftover pivoted columns\n",
    "    remaining = [\n",
    "        c for c in grouped.columns\n",
    "        if c not in desired_columns and c != \"Unit No.\"\n",
    "    ]\n",
    "    final_columns_order = (\n",
    "        [\"Unit No.\"] +\n",
    "        [col for col in desired_columns if col != \"Unit No.\"] +\n",
    "        remaining\n",
    "    )\n",
    "    # Keep only columns that actually exist\n",
    "    final_columns_order = [col for col in final_columns_order if col in grouped.columns]\n",
    "\n",
    "    final_df = grouped[final_columns_order]\n",
    "    \n",
    "    if \"unique_key\" in final_df.columns:\n",
    "        final_df.drop(columns=[\"unique_key\"], inplace=True)\n",
    "\n",
    "    # 9) Convert date columns to YYYY-MM-DD if present\n",
    "    date_cols = [\n",
    "        \"Lease Start Date\",\n",
    "        \"Lease Expiration\",\n",
    "        \"Move In Date\",\n",
    "        \"Move Out Date\",\n",
    "    ]\n",
    "    for col in date_cols:\n",
    "        if col in final_df.columns:\n",
    "            final_df[col] = (\n",
    "                pd.to_datetime(final_df[col], errors=\"coerce\")\n",
    "                .dt.strftime(\"%Y-%m-%d\")\n",
    "            )\n",
    "    final_df = final_df.dropna(subset=[\"Unit No.\"]).reset_index(drop=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3b7e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data_workflow(filepath):\n",
    "    \"\"\"\n",
    "    End-to-end function:\n",
    "    1. Load the Excel file.\n",
    "    2. Identify header candidates.\n",
    "    3. Merge the best header row.\n",
    "    4. Set that row as the header.\n",
    "    5. Standardize headers with GPT.\n",
    "    6. Drop unnecessary rows.\n",
    "    7. Find and apply a \"breaking point.\"\n",
    "    8. Return the final standardized DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Load Excel file\n",
    "    sheet_data = load_excel_file(filepath)\n",
    "    if sheet_data is None or sheet_data.empty:\n",
    "        print(\"No data loaded; exiting workflow.\")\n",
    "        return None\n",
    "    \n",
    "    display_headers_info(sheet_data, \"Original Data:\")\n",
    "    \n",
    "    property_name, as_of_date = get_property_info(filepath)\n",
    "\n",
    "    # Define your keywords\n",
    "    keywords = [\n",
    "        'unit', 'unit id', 'unit number', 'unit no', 'unit designation',\n",
    "        'move-in', 'move in', 'movein', 'move-in date', 'move in date', 'moveindate',\n",
    "        'move-out', 'move out', 'moveout', 'move-out date', 'move out date', 'moveoutdate',\n",
    "        'lease', 'lease start', 'lease start date', 'lease begin', 'start of lease',\n",
    "        'lease end', 'lease end date', 'lease expiration', 'end of lease',\n",
    "        'rent', 'market rent', 'lease rent', 'market + addl.', 'market',\n",
    "        'unit status', 'lease status', 'occupancy', 'unit/lease status',\n",
    "        'floorplan', 'floor plan',\n",
    "        'sqft', 'sq ft', 'square feet', 'square ft', 'square footage', 'sq. ft.', 'sq.ft',\n",
    "        'unit sqft', 'unit size',\n",
    "        'code', 'charge code', 'trans code', 'transaction code', 'description'\n",
    "    ]\n",
    "\n",
    "    # Step 2: Identify header candidates\n",
    "    header_candidates = identify_header_candidates(sheet_data, keywords)\n",
    "    display_headers_info(header_candidates, 'Header Candidates:')\n",
    "\n",
    "    if header_candidates.empty:\n",
    "        print(\"No suitable header rows found; cannot proceed.\")\n",
    "        return None\n",
    "    \n",
    "    # Step 3: Merge and select the best header row\n",
    "    selected_header_df = merge_and_select_first_header_to_bottom(header_candidates, 'keyword_count', keywords)\n",
    "\n",
    "    if selected_header_df.empty:\n",
    "        print(\"No suitable merged header row found; check input file.\")\n",
    "        return None\n",
    "    \n",
    "    # We'll use only the first row from `selected_header_df` for the new header\n",
    "    new_header = selected_header_df.iloc[0, :-1]  # everything except 'keyword_count' column\n",
    "    header_row_index = selected_header_df.index[0]\n",
    "    \n",
    "    # Step 4: Set that row as the header in the main sheet_data\n",
    "    sheet_data.columns = new_header.values\n",
    "    data_start_idx = header_row_index + 1\n",
    "    df = sheet_data[data_start_idx:].reset_index(drop=True)\n",
    "    \n",
    "    display_headers_info(df, \"Data After Setting Headers (Raw):\")\n",
    "    \n",
    "    # Step 5: Set that row as the header in the main sheet_data \n",
    "    df.columns = df.columns.fillna('')  # Convert NaNs in column names to empty strings\n",
    "    empty_name_cols = df.columns[df.columns.str.strip() == '']\n",
    "    if len(empty_name_cols) > 0:\n",
    "        print(f\"Detected columns with empty names: {list(empty_name_cols)}\")\n",
    "        df.drop(columns=empty_name_cols, inplace=True)\n",
    "        display_headers_info(df, \"Data After Dropping Empty Column Names:\")\n",
    "\n",
    "\n",
    "    # Step 6: Standardize headers with GPT\n",
    "    # Prepare the prompt\n",
    "    instructions_prompt = standardization_instructions()\n",
    "    # Original list of headers\n",
    "    original_headers = list(df.columns)\n",
    "    # Get standardized headers from GPT\n",
    "    standardized_headers = standardize_headers_with_retries(original_headers, instructions_prompt, client)\n",
    "    # Make them unique if duplicates\n",
    "    standardized_headers = make_column_names_unique(standardized_headers)\n",
    "    # Assign to DataFrame\n",
    "    df.columns = standardized_headers\n",
    "    \n",
    "    display_headers_info(df, \"DataFrame After GPT Header Standardization:\")\n",
    "\n",
    "    # Step 7: Drop rows that contain no numeric data\n",
    "    df = drop_unnecessary_rows(df)\n",
    "    \n",
    "    display_headers_info(df, \"DataFrame After Dropping Unnecessary Rows:\")\n",
    "\n",
    "    # Step 8: Find the \"breaking point\" and keep only rows up to that point\n",
    "    breaking_point = find_breaking_point(df)\n",
    "    if breaking_point is not None:\n",
    "        df = df[:breaking_point]\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        print(f\"Applied breaking point at row: {breaking_point}\")\n",
    "\n",
    "    display_headers_info(df, \"Final Standardized Data:\")\n",
    "\n",
    "    # Step 9: Processing multiline files\n",
    "    df = finalize_columns(df)\n",
    "    display_headers_info(df, \"Data After Finalizing Columns with Date Formatting:\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b326dad9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rent Roll with Lease Charges</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hurstbourne Estates (hurstbou)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As Of = 10/25/2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Month Year = 10/2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unit</td>\n",
       "      <td>Unit Type</td>\n",
       "      <td>Unit</td>\n",
       "      <td>Resident</td>\n",
       "      <td>Name</td>\n",
       "      <td>Market</td>\n",
       "      <td>Charge</td>\n",
       "      <td>Amount</td>\n",
       "      <td>Resident</td>\n",
       "      <td>Other</td>\n",
       "      <td>Move In</td>\n",
       "      <td>Lease</td>\n",
       "      <td>Move Out</td>\n",
       "      <td>Balance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "Shape: (1495, 14)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Property Name: Hurstbourne Estates\n",
      "As of date: 2024-10-25\n",
      "Header Candidates:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>keyword_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>unit</td>\n",
       "      <td>unit type</td>\n",
       "      <td>unit</td>\n",
       "      <td>resident</td>\n",
       "      <td>name</td>\n",
       "      <td>market</td>\n",
       "      <td>charge</td>\n",
       "      <td>amount</td>\n",
       "      <td>resident</td>\n",
       "      <td>other</td>\n",
       "      <td>move in</td>\n",
       "      <td>lease</td>\n",
       "      <td>move out</td>\n",
       "      <td>balance</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>sq ft</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>rent</td>\n",
       "      <td>code</td>\n",
       "      <td></td>\n",
       "      <td>deposit</td>\n",
       "      <td>deposit</td>\n",
       "      <td></td>\n",
       "      <td>expiration</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>summary groups</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>square</td>\n",
       "      <td>market</td>\n",
       "      <td>lease</td>\n",
       "      <td>security</td>\n",
       "      <td>other</td>\n",
       "      <td># of</td>\n",
       "      <td>% unit</td>\n",
       "      <td>% sqft</td>\n",
       "      <td>balance</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>footage</td>\n",
       "      <td>rent</td>\n",
       "      <td>charges</td>\n",
       "      <td>deposit</td>\n",
       "      <td>deposits</td>\n",
       "      <td>units</td>\n",
       "      <td>occupancy</td>\n",
       "      <td>occupied</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 'keyword_count']\n",
      "Shape: (4, 15)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Data After Setting Headers (Raw):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit</th>\n",
       "      <th>unit type</th>\n",
       "      <th>unit sq ft</th>\n",
       "      <th>resident</th>\n",
       "      <th>name</th>\n",
       "      <th>market rent</th>\n",
       "      <th>charge code</th>\n",
       "      <th>amount</th>\n",
       "      <th>resident deposit</th>\n",
       "      <th>other deposit</th>\n",
       "      <th>move in</th>\n",
       "      <th>lease expiration</th>\n",
       "      <th>move out</th>\n",
       "      <th>balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Current/Notice/Vacant Residents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-101</td>\n",
       "      <td>hb-2d</td>\n",
       "      <td>1097</td>\n",
       "      <td>t0101126</td>\n",
       "      <td>Larry Blankenship</td>\n",
       "      <td>1632</td>\n",
       "      <td>rent</td>\n",
       "      <td>1074</td>\n",
       "      <td>1390</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-03-27 00:00:00</td>\n",
       "      <td>2025-04-26 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>garage</td>\n",
       "      <td>95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAC</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mediapac</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['unit ', 'unit type ', 'unit sq ft', 'resident ', 'name ', 'market rent', 'charge code', 'amount ', 'resident deposit', 'other deposit', 'move in ', 'lease expiration', 'move out ', 'balance ']\n",
      "Shape: (1489, 14)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "DataFrame After GPT Header Standardization:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unit No.</th>\n",
       "      <th>Floor Plan Code</th>\n",
       "      <th>Net sf</th>\n",
       "      <th>Resident</th>\n",
       "      <th>Name</th>\n",
       "      <th>Market Rent</th>\n",
       "      <th>Charge Codes</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Resident Deposit</th>\n",
       "      <th>Other Deposit</th>\n",
       "      <th>Move In Date</th>\n",
       "      <th>Lease Expiration</th>\n",
       "      <th>Move Out Date</th>\n",
       "      <th>Balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Current/Notice/Vacant Residents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-101</td>\n",
       "      <td>hb-2d</td>\n",
       "      <td>1097</td>\n",
       "      <td>t0101126</td>\n",
       "      <td>Larry Blankenship</td>\n",
       "      <td>1632</td>\n",
       "      <td>rent</td>\n",
       "      <td>1074</td>\n",
       "      <td>1390</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-03-27 00:00:00</td>\n",
       "      <td>2025-04-26 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>garage</td>\n",
       "      <td>95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAC</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mediapac</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['Unit No.', 'Floor Plan Code', 'Net sf', 'Resident', 'Name', 'Market Rent', 'Charge Codes', 'Amount', 'Resident Deposit', 'Other Deposit', 'Move In Date', 'Lease Expiration', 'Move Out Date', 'Balance']\n",
      "Shape: (1489, 14)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Dropped 293 rows that contained no numeric values.\n",
      "DataFrame After Dropping Unnecessary Rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unit No.</th>\n",
       "      <th>Floor Plan Code</th>\n",
       "      <th>Net sf</th>\n",
       "      <th>Resident</th>\n",
       "      <th>Name</th>\n",
       "      <th>Market Rent</th>\n",
       "      <th>Charge Codes</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Resident Deposit</th>\n",
       "      <th>Other Deposit</th>\n",
       "      <th>Move In Date</th>\n",
       "      <th>Lease Expiration</th>\n",
       "      <th>Move Out Date</th>\n",
       "      <th>Balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-101</td>\n",
       "      <td>hb-2d</td>\n",
       "      <td>1097</td>\n",
       "      <td>t0101126</td>\n",
       "      <td>Larry Blankenship</td>\n",
       "      <td>1632</td>\n",
       "      <td>rent</td>\n",
       "      <td>1074</td>\n",
       "      <td>1390</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-03-27 00:00:00</td>\n",
       "      <td>2025-04-26 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>garage</td>\n",
       "      <td>95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAC</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mediapac</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Total</td>\n",
       "      <td>1262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['Unit No.', 'Floor Plan Code', 'Net sf', 'Resident', 'Name', 'Market Rent', 'Charge Codes', 'Amount', 'Resident Deposit', 'Other Deposit', 'Move In Date', 'Lease Expiration', 'Move Out Date', 'Balance']\n",
      "Shape: (1196, 14)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Applied breaking point at row: 1180\n",
      "Final Standardized Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unit No.</th>\n",
       "      <th>Floor Plan Code</th>\n",
       "      <th>Net sf</th>\n",
       "      <th>Resident</th>\n",
       "      <th>Name</th>\n",
       "      <th>Market Rent</th>\n",
       "      <th>Charge Codes</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Resident Deposit</th>\n",
       "      <th>Other Deposit</th>\n",
       "      <th>Move In Date</th>\n",
       "      <th>Lease Expiration</th>\n",
       "      <th>Move Out Date</th>\n",
       "      <th>Balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-101</td>\n",
       "      <td>hb-2d</td>\n",
       "      <td>1097</td>\n",
       "      <td>t0101126</td>\n",
       "      <td>Larry Blankenship</td>\n",
       "      <td>1632</td>\n",
       "      <td>rent</td>\n",
       "      <td>1074</td>\n",
       "      <td>1390</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-03-27 00:00:00</td>\n",
       "      <td>2025-04-26 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>garage</td>\n",
       "      <td>95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAC</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mediapac</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Total</td>\n",
       "      <td>1262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['Unit No.', 'Floor Plan Code', 'Net sf', 'Resident', 'Name', 'Market Rent', 'Charge Codes', 'Amount', 'Resident Deposit', 'Other Deposit', 'Move In Date', 'Lease Expiration', 'Move Out Date', 'Balance']\n",
      "Shape: (1180, 14)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Detected scenario with 'Charge Codes' & 'Amount'. We will pivot them.\n",
      "Data After Finalizing Columns with Date Formatting:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unit No.</th>\n",
       "      <th>Floor Plan Code</th>\n",
       "      <th>Net sf</th>\n",
       "      <th>Occupancy Status / Code</th>\n",
       "      <th>Enter \"F\" for Future Lease</th>\n",
       "      <th>Market Rent</th>\n",
       "      <th>Lease Start Date</th>\n",
       "      <th>Lease Expiration</th>\n",
       "      <th>Lease Term (months)</th>\n",
       "      <th>Move In Date</th>\n",
       "      <th>Move Out Date</th>\n",
       "      <th>Resident</th>\n",
       "      <th>Name</th>\n",
       "      <th>Resident Deposit</th>\n",
       "      <th>Other Deposit</th>\n",
       "      <th>Balance</th>\n",
       "      <th>CAC</th>\n",
       "      <th>Total</th>\n",
       "      <th>conc</th>\n",
       "      <th>empdisc</th>\n",
       "      <th>garage</th>\n",
       "      <th>mediapac</th>\n",
       "      <th>pestctrl</th>\n",
       "      <th>petrent</th>\n",
       "      <th>rent</th>\n",
       "      <th>storage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-101</td>\n",
       "      <td>hb-2d</td>\n",
       "      <td>1097</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-04-26</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t0101126</td>\n",
       "      <td>Larry Blankenship</td>\n",
       "      <td>1390</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1074</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-102</td>\n",
       "      <td>hb-1n</td>\n",
       "      <td>952</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-19</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-08-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t0158974</td>\n",
       "      <td>Cassandra Forker</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>-186</td>\n",
       "      <td>20</td>\n",
       "      <td>1494</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>1381</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-103</td>\n",
       "      <td>hb-1h</td>\n",
       "      <td>783</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-01-17</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-12-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t0173231</td>\n",
       "      <td>Raymond Tri</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1114</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-104</td>\n",
       "      <td>hb-1l</td>\n",
       "      <td>865</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-02-20</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-12-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hb447733</td>\n",
       "      <td>Michael Griffin</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>1249</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-105</td>\n",
       "      <td>hb-1h</td>\n",
       "      <td>783</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-01-05</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t0163231</td>\n",
       "      <td>AHI Housing</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>2277</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2184</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['Unit No.', 'Floor Plan Code', 'Net sf', 'Occupancy Status / Code', 'Enter \"F\" for Future Lease', 'Market Rent', 'Lease Start Date', 'Lease Expiration', 'Lease Term (months)', 'Move In Date', 'Move Out Date', 'Resident', 'Name', 'Resident Deposit', 'Other Deposit', 'Balance', 'CAC', 'Total', 'conc', 'empdisc', 'garage', 'mediapac', 'pestctrl', 'petrent', 'rent', 'storage']\n",
      "Shape: (281, 26)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath = \"Test_Original_Files/Original_Files/280815 - Original Upload.xlsx\"\n",
    "#\n",
    "final_df = standardize_data_workflow(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ab9c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    \"\"\"Process a single row and determine occupancy status.\"\"\"\n",
    "    # Prepare the row data as a string for the API\n",
    "    row_data = row.to_dict()\n",
    "    row_text = \"\\n\".join([f\"{key}: {value}\" for key, value in row_data.items()])\n",
    "    \n",
    "    # Prompt for API\n",
    "    prompt = f\"\"\"\n",
    "    You are a data processing assistant. I will provide you with rows of data from a CSV file, each representing information about a rental unit. Your task is to categorize the Occupancy Status / Code for each row based on these rules:\n",
    "        1.Occupied: The row contains a tenant name, and at least one of the following fields: lease start date, lease expiration date, or move-in date. Additional charges may also be present along with the market rent.\n",
    "        2.Vacant: The tenant name is absent or displays ‘Vacant’. Lease-related fields (dates) are empty. Market rent is present, but no other charges exist.\n",
    "        3.Model: Similar to Vacant, but the tenant name displays ‘Model’. Market rent is present, but no other charges or lease related information.\n",
    "        4.Applicant: A tenant name is present and lease-related fields (e.g., dates) may be present. Only market rent is listed; no other charges are included.\n",
    "    Based on the following data, determine the `Occupancy Status / Code` using the rules provided earlier:\n",
    "    {row_text}\n",
    "    Only return the category as the output (Occupied, Vacant, Model, or Applicant).\n",
    "    \n",
    "    Note current, notice are considered as Occupied\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call the OpenAI API\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        # Extract the response\n",
    "        status = response.choices[0].message.content.strip()\n",
    "        return status\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "\n",
    "def label_occupancy_status_parallel(df, max_workers):\n",
    "    \"\"\"\n",
    "    Label occupancy status for rows in a DataFrame using parallel processing.\n",
    "    \"\"\"\n",
    "    # Apply parallel processing\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = list(executor.map(process_row, [row for _, row in df.iterrows()]))\n",
    "    \n",
    "    # Add results to the DataFrame\n",
    "    df['Occupancy Status / Code'] = results\n",
    "    return df\n",
    "\n",
    "def identify_pairs(df):\n",
    "    \"\"\"\n",
    "    Identify pairs of rows with the same unit ID and set their Occupancy Status to None.\n",
    "    \"\"\"\n",
    "    # Group by unit ID and get groups with two or more rows\n",
    "    grouped = df.groupby('Unit No.')\n",
    "    eligible_unit_ids = grouped.filter(lambda x: len(x) >= 2)['Unit No.'].unique()\n",
    "\n",
    "    # Set Occupancy Status to None for rows in eligible groups\n",
    "    df.loc[df['Unit No.'].isin(eligible_unit_ids), \"Occupancy Status / Code\"] = None\n",
    "\n",
    "    # Extract pairs as subsets\n",
    "    pairs = [group for _, group in grouped if len(group) >= 2]\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def process_pair(pair, property_name, as_of_date):\n",
    "    \"\"\"\n",
    "    Process a pair of rows and determine which is Vacant and which is Applicant.\n",
    "    \"\"\"\n",
    "    \n",
    "    pair_text = \"\\n\\n\".join(\n",
    "        f\"Row {i + 1}:\\n\" + \"\\n\".join([f\"{key}: {value}\" for key, value in row.items()])\n",
    "        for i, row in enumerate(pair.to_dict(orient=\"records\"))\n",
    "    )\n",
    "    \n",
    "    #print(pair_text)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "\n",
    "        You are a data processing assistant tasked with determining the \"Occupancy Status / Code\" for rows representing the same unit in a property data file.\n",
    "\n",
    "        Key Information:\n",
    "        Property Name: {property_name}\n",
    "        Reference Date: {as_of_date}\n",
    "\n",
    "        Occupancy Status Definitions:\n",
    "        You cannot have same status for same unit no make your best guess out of the two pairs.\n",
    "        Assign one status per row using the definitions and inference rules. \n",
    "\n",
    "\n",
    "        Vacant: No occupant name or explicitly listed as \"Vacant.\"\n",
    "        Applicant: Occupant name present, and dates (move-in/lease start) are future or near the present date to {as_of_date}.\n",
    "        Occupied: Occupant name present, and:\n",
    "        Date is < {as_of_date}, with rent charges indicating an active/historical lease.\n",
    "        Special Cases:\n",
    "        Units may have multiple rows reflecting transitions (e.g., Occupied → Vacant → Applicant).\n",
    "        Inference Rule: Rows for the same unit should not have the same status. Use transitions to guide decisions (e.g., if one is Occupied, the other is likely Applicant or Vacant).\n",
    "\n",
    "        For each row, analyze: Occupant name, relevant dates (move-in/out, lease start), rent charges, and {as_of_date}.\n",
    "        \n",
    "        Format:\n",
    "        Respond with one status per row:\n",
    "        Row 1: <status>\n",
    "        Row 2: <status>\n",
    "\n",
    "        Data rows:\n",
    "        {pair_text}\n",
    "\n",
    "        Please respond with one status per row, in the format:\n",
    "        Row 1: <status>\n",
    "        Row 2: <status>\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        # Parse the response\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        statuses = {\n",
    "            line.split(\":\")[0].strip(): line.split(\":\")[1].strip()\n",
    "            for line in result.split(\"\\n\") if \":\" in line\n",
    "        }\n",
    "        return statuses\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing pair: {e}\")\n",
    "        return {\"Row 1\": \"Error\", \"Row 2\": \"Error\"}\n",
    "\n",
    "\n",
    "def refine_pairs(df, pairs, property_name, as_of_date, max_retries=3):\n",
    "    \"\"\"\n",
    "    Refine the occupancy status for rows with paired unit IDs, with retries for failed updates.\n",
    "    \"\"\"\n",
    "    for pair in pairs:\n",
    "        retries = 0\n",
    "        success = False\n",
    "\n",
    "        while retries < max_retries and not success:\n",
    "            # Process the pair through the LLM\n",
    "            statuses = process_pair(pair, property_name, as_of_date)\n",
    "\n",
    "            # Check if statuses are valid (non-null for all rows)\n",
    "            if all(statuses.get(f\"Row {i + 1}\", None) for i in range(len(pair))):\n",
    "                # Update the DataFrame\n",
    "                for i, (index, row) in enumerate(pair.iterrows()):\n",
    "                    row_name = f\"Row {i + 1}\"\n",
    "                    if row_name in statuses:\n",
    "                        df.at[index, \"Occupancy Status / Code\"] = statuses[row_name]\n",
    "                success = True\n",
    "            else:\n",
    "                retries += 1\n",
    "                print(f\"Retry {retries} for pair:\\n{pair}\")\n",
    "\n",
    "        # If retries fail, set a default status\n",
    "        if not success:\n",
    "            print(f\"Failed to update statuses for pair after {max_retries} retries:\\n{pair}\")\n",
    "            for i, (index, row) in enumerate(pair.iterrows()):\n",
    "                df.at[index, \"Occupancy Status / Code\"] = \"Error\"  # Or use a more specific fallback status\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def refine_occupancy_status(df, max_workers, property_name, as_of_date, max_retries=3):\n",
    "    \"\"\"\n",
    "    Full workflow to label occupancy status and refine pairs with retries.\n",
    "    \"\"\"\n",
    "    # First pass: initial categorization\n",
    "    df = label_occupancy_status_parallel(df, max_workers)\n",
    "\n",
    "    # Identify pairs\n",
    "    pairs = identify_pairs(df)\n",
    "\n",
    "    # Refine the pairs with retries\n",
    "    df = refine_pairs(df, pairs, property_name, as_of_date, max_retries=max_retries)\n",
    "    \n",
    "    # Update future lease indicator for Applicants\n",
    "    df.loc[df['Occupancy Status / Code'] == 'Applicant', 'Enter \"F\" for Future Lease'] = 'F'\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "193e5268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property Name: Hurstbourne Estates\n",
      "As of date: 2024-10-25\n",
      "Retry 1 for pair:\n",
      "   Unit No. Floor Plan Code Net sf Occupancy Status / Code  \\\n",
      "24   02-111           hb-1f    762                    None   \n",
      "25   02-111           hb-1f    762                    None   \n",
      "\n",
      "   Enter \"F\" for Future Lease Market Rent Lease Start Date Lease Expiration  \\\n",
      "24                       None        1177              NaN       2024-11-02   \n",
      "25                       None        1177              NaN       2025-12-28   \n",
      "\n",
      "   Lease Term (months) Move In Date  ...  CAC Total conc empdisc garage  \\\n",
      "24                None   2023-11-03  ...   15  1175  NaN     NaN    NaN   \n",
      "25                None   2024-11-29  ...  NaN     0  NaN     NaN    NaN   \n",
      "\n",
      "   mediapac pestctrl petrent  rent storage  \n",
      "24       73      NaN     NaN  1087     NaN  \n",
      "25      NaN      NaN     NaN   NaN     NaN  \n",
      "\n",
      "[2 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "property_name, as_of_date = get_property_info(filepath)\n",
    "\n",
    "updated_df = refine_occupancy_status(final_df, max_workers=50, \n",
    "                                     property_name=property_name, as_of_date=as_of_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "439f96cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Occupancy Status / Code\n",
       "Occupied     262\n",
       "Applicant      9\n",
       "Vacant         9\n",
       "Model          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df['Occupancy Status / Code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46151fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d9b31e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_to_drop(columns):\n",
    "    \"\"\"\n",
    "    Use an LLM to determine which columns should be dropped from the DataFrame.\n",
    "    \"\"\"\n",
    "    # Generate the prompt\n",
    "    column_list = \", \".join(repr(col) for col in columns)\n",
    "    instructions_prompt = \"\"\"\n",
    "    You are a data processing assistant. I will provide you with a list of column names from a dataset \n",
    "    representing rental unit information. Your task is to determine which columns should be dropped \n",
    "    based on the following high-level rules:\n",
    "\n",
    "    **Rules for Dropping Columns**:\n",
    "    1. Do not keep any tenant-specific information:\n",
    "        - Examples: Tenant name, Tenant ID, or any other identifying information.\n",
    "    2. Do not keep totals, balances, or outstanding amounts:\n",
    "        - Examples: Totals, total charges, balance owed, deposits etc.\n",
    "    3. Do not keep calculated metrics per square footage that are not directly needed:\n",
    "    4. Be conservative:\n",
    "        - Keep more columns rather than accidentally dropping something essential.\n",
    "        - If in doubt, retain the column. comrent keep\n",
    "\n",
    "    **Columns to Keep**:\n",
    "    - Columns related to property information:\n",
    "        - Examples: Market rent, rent, lease start date, lease expiration date, move-in date, unit details (e.g., net square footage), misc, trash, pet\n",
    "    - Columns providing essential context for the dataset.\n",
    "\n",
    "    Return the response as a JSON object with the following format:\n",
    "    {{\n",
    "        \"dropped_columns\": [\"Column 1\", \"Column 2\", \"Column 3\"]\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call the GPT model\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Columns: {column_list}\"}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        response_format={ \"type\": \"json_object\" }\n",
    "\n",
    "    )\n",
    "\n",
    "    response_content = response.choices[0].message.content\n",
    "    try:\n",
    "        dropped_columns = json.loads(response_content)['dropped_columns']\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        print(\"Raw response from LLM:\", response_content)\n",
    "        try:\n",
    "            dropped_columns = eval(response_content)\n",
    "        except:\n",
    "            dropped_columns = None\n",
    "\n",
    "    return dropped_columns\n",
    "\n",
    "\n",
    "def drop_unnecessary_columns(df, columns_to_drop):\n",
    "    \"\"\"\n",
    "    Drop the specified columns from the DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "    return df\n",
    "\n",
    "\n",
    "def refine_dataframe(df):\n",
    "    \"\"\"\n",
    "    Refine the DataFrame by identifying and dropping unnecessary columns using an LLM.\n",
    "    \"\"\"\n",
    "    columns = df.columns.tolist()\n",
    "    columns_to_drop = get_columns_to_drop(columns)\n",
    "    \n",
    "    # Ensure columns_to_drop is a valid list\n",
    "    if not columns_to_drop:\n",
    "        columns_to_drop = []\n",
    "\n",
    "    # Now columns_to_drop is an empty list if it was None\n",
    "    print(\"Columns to drop:\", columns_to_drop)\n",
    "    \n",
    "    refined_df = drop_unnecessary_columns(df, columns_to_drop)\n",
    "    return refined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14b914eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to drop: ['Resident', 'Name', 'Resident Deposit', 'Other Deposit', 'Balance', 'Total']\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = refine_dataframe(updated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27012659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unit No.</th>\n",
       "      <th>Floor Plan Code</th>\n",
       "      <th>Net sf</th>\n",
       "      <th>Occupancy Status / Code</th>\n",
       "      <th>Enter \"F\" for Future Lease</th>\n",
       "      <th>Market Rent</th>\n",
       "      <th>Lease Start Date</th>\n",
       "      <th>Lease Expiration</th>\n",
       "      <th>Lease Term (months)</th>\n",
       "      <th>Move In Date</th>\n",
       "      <th>Move Out Date</th>\n",
       "      <th>CAC</th>\n",
       "      <th>conc</th>\n",
       "      <th>empdisc</th>\n",
       "      <th>garage</th>\n",
       "      <th>mediapac</th>\n",
       "      <th>pestctrl</th>\n",
       "      <th>petrent</th>\n",
       "      <th>rent</th>\n",
       "      <th>storage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-101</td>\n",
       "      <td>hb-2d</td>\n",
       "      <td>1097</td>\n",
       "      <td>Occupied</td>\n",
       "      <td>None</td>\n",
       "      <td>1632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-04-26</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1074</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-102</td>\n",
       "      <td>hb-1n</td>\n",
       "      <td>952</td>\n",
       "      <td>Occupied</td>\n",
       "      <td>None</td>\n",
       "      <td>1152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-19</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-08-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>1381</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-103</td>\n",
       "      <td>hb-1h</td>\n",
       "      <td>783</td>\n",
       "      <td>Occupied</td>\n",
       "      <td>None</td>\n",
       "      <td>1179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-01-17</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-12-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1114</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-104</td>\n",
       "      <td>hb-1l</td>\n",
       "      <td>865</td>\n",
       "      <td>Occupied</td>\n",
       "      <td>None</td>\n",
       "      <td>1149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-02-20</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-12-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>1249</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-105</td>\n",
       "      <td>hb-1h</td>\n",
       "      <td>783</td>\n",
       "      <td>Occupied</td>\n",
       "      <td>None</td>\n",
       "      <td>1179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-01-05</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2184</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>1804-306</td>\n",
       "      <td>hb-2j</td>\n",
       "      <td>1265</td>\n",
       "      <td>Occupied</td>\n",
       "      <td>None</td>\n",
       "      <td>1410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-05-28</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-07-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1726</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>1804-307</td>\n",
       "      <td>hb-2g</td>\n",
       "      <td>1176</td>\n",
       "      <td>Vacant</td>\n",
       "      <td>None</td>\n",
       "      <td>1259</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>1804-308</td>\n",
       "      <td>hb-1j</td>\n",
       "      <td>824</td>\n",
       "      <td>Occupied</td>\n",
       "      <td>None</td>\n",
       "      <td>1132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-08-08</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-08-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1196</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>1804-309</td>\n",
       "      <td>hb-1</td>\n",
       "      <td>663</td>\n",
       "      <td>Occupied</td>\n",
       "      <td>None</td>\n",
       "      <td>1193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-07-06</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-06-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1318</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>1804-310</td>\n",
       "      <td>hb-2b</td>\n",
       "      <td>1093</td>\n",
       "      <td>Occupied</td>\n",
       "      <td>None</td>\n",
       "      <td>1461</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-10-15</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-10-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>1308</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unit No. Floor Plan Code Net sf Occupancy Status / Code  \\\n",
       "0      01-101           hb-2d   1097                Occupied   \n",
       "1      01-102           hb-1n    952                Occupied   \n",
       "2      01-103           hb-1h    783                Occupied   \n",
       "3      01-104           hb-1l    865                Occupied   \n",
       "4      01-105           hb-1h    783                Occupied   \n",
       "..        ...             ...    ...                     ...   \n",
       "276  1804-306           hb-2j   1265                Occupied   \n",
       "277  1804-307           hb-2g   1176                  Vacant   \n",
       "278  1804-308           hb-1j    824                Occupied   \n",
       "279  1804-309            hb-1    663                Occupied   \n",
       "280  1804-310           hb-2b   1093                Occupied   \n",
       "\n",
       "    Enter \"F\" for Future Lease Market Rent Lease Start Date Lease Expiration  \\\n",
       "0                         None        1632              NaN       2025-04-26   \n",
       "1                         None        1152              NaN       2025-09-19   \n",
       "2                         None        1179              NaN       2025-01-17   \n",
       "3                         None        1149              NaN       2025-02-20   \n",
       "4                         None        1179              NaN       2025-01-05   \n",
       "..                         ...         ...              ...              ...   \n",
       "276                       None        1410              NaN       2025-05-28   \n",
       "277                       None        1259              NaN              NaN   \n",
       "278                       None        1132              NaN       2025-08-08   \n",
       "279                       None        1193              NaN       2025-07-06   \n",
       "280                       None        1461              NaN       2025-10-15   \n",
       "\n",
       "    Lease Term (months) Move In Date Move Out Date  CAC conc empdisc garage  \\\n",
       "0                  None   2020-03-27           NaN   20  NaN     NaN     95   \n",
       "1                  None   2022-08-20           NaN   20  NaN     NaN    NaN   \n",
       "2                  None   2023-12-18           NaN   20  NaN     NaN    NaN   \n",
       "3                  None   2018-12-21           NaN   20  NaN     NaN    NaN   \n",
       "4                  None   2023-04-06           NaN   20  NaN     NaN    NaN   \n",
       "..                  ...          ...           ...  ...  ...     ...    ...   \n",
       "276                None   2023-07-29           NaN   20  NaN     NaN    NaN   \n",
       "277                None          NaN           NaN  NaN  NaN     NaN    NaN   \n",
       "278                None   2024-08-09           NaN   20  NaN     NaN    NaN   \n",
       "279                None   2024-06-07           NaN   20  NaN     NaN    NaN   \n",
       "280                None   2023-10-16           NaN   20  NaN     NaN    NaN   \n",
       "\n",
       "    mediapac pestctrl petrent  rent storage  \n",
       "0         73      NaN     NaN  1074     NaN  \n",
       "1         73      NaN      20  1381     NaN  \n",
       "2         73      NaN     NaN  1114     NaN  \n",
       "3         73      NaN      20  1249     NaN  \n",
       "4         73      NaN     NaN  2184     NaN  \n",
       "..       ...      ...     ...   ...     ...  \n",
       "276       73      NaN     NaN  1726     NaN  \n",
       "277      NaN      NaN     NaN   NaN     NaN  \n",
       "278       73      NaN     NaN  1196     NaN  \n",
       "279       73      NaN     NaN  1318     NaN  \n",
       "280       73      NaN      50  1308     NaN  \n",
       "\n",
       "[281 rows x 20 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a5d31bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_file(df, original_filepath, root_folder=\"Test_Original_Files\", processed_subfolder=\"AI_processed_files\"):\n",
    "    # Ensure the original file is within the specified root folder\n",
    "    if not os.path.abspath(original_filepath).startswith(os.path.abspath(root_folder)):\n",
    "        raise ValueError(f\"Original file must be located under {root_folder}\")\n",
    "    \n",
    "    base_name = os.path.basename(original_filepath)  \n",
    "    file_stem, ext = os.path.splitext(base_name)     \n",
    "    \n",
    "    output_folder = os.path.join(root_folder, processed_subfolder)\n",
    "    \n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    new_file_name = f\"{file_stem}_processed{ext}\"  \n",
    "    \n",
    "    out_path = os.path.join(output_folder, new_file_name)\n",
    "    \n",
    "    df.to_excel(out_path, index=False)\n",
    "    print(f\"File saved to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8250c476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: Test_Original_Files/AI_processed_files/280815 - Original Upload_processed.xlsx\n"
     ]
    }
   ],
   "source": [
    "#cleaned_df.to_excel('processed_280815_AK.xlsx', index=False)\n",
    "save_processed_file(cleaned_df, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20188b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
